{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior Elicitation\n",
    "\n",
    "\n",
    "A key paper in this area from [Mikkola et al. 2024](https://projecteuclid.org/journals/bayesian-analysis/volume-19/issue-4/Prior-Knowledge-Elicitation-The-Past-Present-and-Future/10.1214/23-BA1381.full).\n",
    "\n",
    "And a great blog post from [Michael Bettancourt](https://betanalpha.github.io/assets/case_studies/prior_modeling.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whence piors?\n",
    "\n",
    "A key question in Bayesian modelling lies in what are priors anyhow? What do they represent? The most succinct definition is that priors are representations of our personal beliefs. But how can we do that? It seems both sensible and nonsense at the same time\n",
    "\n",
    "> \"...statistics are always to some extent constructed on the basis of judgements, and it would be an obvious delusion to think the full complexity of personal experience can be unambiguously coded and put into a spreadsheet or other software.\" -- David Spiegelhalter\n",
    "\n",
    "And yet this is ultimately what Bayes theorem demands of us\n",
    "\n",
    "> \"In Bayesian theory, a 'prior' represents one's personal degree of belief before considering current evidence.\" -- ET Jaynes\n",
    "\n",
    "So if this is the case, and we wish to set priors in a principled way, how can we go about it? How should we go about specifying our own priors? And how can we specify the priors of others? How do we translate our intuitive sense of plausibility into a probability distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import scipy as sp\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Triangular Distribution\n",
    "\n",
    "When we ask students (or domain experts) about a question, such as \"what percentage of the earth does the Atlantic Ocean cover\", we typically don't get a single number. Instead, we might hear:\n",
    "\n",
    "> *\"I'm guessing it's around 15% of Earth's surface, but it could be as low as 10% or as high as 30%\"*\n",
    "\n",
    "This gives us three key values:\n",
    "- **Minimum** (a): The lowest plausible value (10%)\n",
    "- **Mode** (c): The most likely value (15%)\n",
    "- **Maximum** (b): The highest plausible value (30%)\n",
    "\n",
    "The **triangular distribution** is perfect for capturing this kind of belief. It is useful for elicitation because it requires only three intuitive parameters: a minimum, a maximum, and a mode (most likely value). It has been widely used in conservation decision making (a niche, but very interesting field): https://scholar.google.com/citations?user=O4YYKCsAAAAJ&hl=en&oi=ao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elicited parameters from our hypothetical person\n",
    "min_val = 0.10  # 10% minimum\n",
    "mode_val = 0.15  # 15% most likely\n",
    "max_val = 0.30  # 30% maximum\n",
    "\n",
    "# The triangular distribution in scipy needs the mode scaled between 0 and 1\n",
    "# c = (mode - min) / (max - min)\n",
    "c_param = (mode_val - min_val) / (max_val - min_val)\n",
    "\n",
    "# Create the triangular distribution\n",
    "tri_dist = stats.triang(c=c_param, loc=min_val, scale=max_val - min_val)\n",
    "\n",
    "print(f\"Elicited prior for Atlantic Ocean coverage:\")\n",
    "print(f\"  Minimum: {min_val*100:.0f}%\")\n",
    "print(f\"  Mode:    {mode_val*100:.0f}%\")\n",
    "print(f\"  Maximum: {max_val*100:.0f}%\")\n",
    "print(f\"\\nTriangular distribution parameters:\")\n",
    "print(f\"  c = {c_param:.3f}, loc = {min_val:.2f}, scale = {max_val - min_val:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So using the `scipy` package in python, we get out parameters from our input min, mode, max values, namely c (the location of the mode between the minimum and maximum values), loc (minimum value), and scale (the range of the distribution). These are most easily understood visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the elicited prior\n",
    "x = np.linspace(0, 1, 1000)\n",
    "y = tri_dist.pdf(x)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x, y, 'b-', linewidth=2, label='Triangular prior')\n",
    "plt.axvline(min_val, color='red', linestyle='--', alpha=0.5, label=f'Min: {min_val*100:.0f}%')\n",
    "plt.axvline(mode_val, color='green', linestyle='--', alpha=0.5, label=f'Mode: {mode_val*100:.0f}%')\n",
    "plt.axvline(max_val, color='red', linestyle='--', alpha=0.5, label=f'Max: {max_val*100:.0f}%')\n",
    "plt.fill_between(x, y, alpha=0.3)\n",
    "plt.xlabel('Proportion of Earth covered by Atlantic Ocean', fontsize=12)\n",
    "plt.ylabel('Probability density', fontsize=12)\n",
    "#plt.title('Elicited Prior: Triangular Distribution', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.xlim(0, 1)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig('triangular.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics of our elicited prior\n",
    "tri_mean = tri_dist.mean()\n",
    "tri_std = tri_dist.std()\n",
    "tri_median = tri_dist.median()\n",
    "\n",
    "print(f\"Summary statistics of triangular prior:\")\n",
    "print(f\"  Mean:   {tri_mean:.3f} ({tri_mean*100:.1f}%)\")\n",
    "print(f\"  Median: {tri_median:.3f} ({tri_median*100:.1f}%)\")\n",
    "print(f\"  Std:    {tri_std:.3f} ({tri_std*100:.1f}%)\")\n",
    "print(f\"\\nNote: For triangular distributions, the mean is (a + b + c)/3\")\n",
    "print(f\"      Calculated: ({min_val:.2f} + {max_val:.2f} + {mode_val:.2f})/3 = {(min_val + max_val + mode_val)/3:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics of the Triangular Distribution\n",
    "\n",
    "For a triangular distribution with parameters:\n",
    "- $\\text{min}$ = minimum value\n",
    "- $\\text{max}$ = maximum value  \n",
    "- $\\text{mode}$ = most likely value\n",
    "\n",
    "### Mean (Expected Value)\n",
    "\n",
    "The mean of a triangular distribution is simply the average of the three parameters:\n",
    "\n",
    "$$\\mu = \\frac{\\text{min} + \\text{max} + \\text{mode}}{3}$$\n",
    "\n",
    "**Intuition**: The mean balances the three \"anchor points\" equally. This differs from the uniform distribution where the mean would be just $\\frac{\\text{min} + \\text{max}}{2}$.\n",
    "\n",
    "**Example**: For $\\text{min} = 0.10$, $\\text{mode} = 0.15$, $\\text{max} = 0.30$:\n",
    "\n",
    "$$\\mu = \\frac{0.10 + 0.30 + 0.15}{3} = \\frac{0.55}{3} = 0.183$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the elicited prior\n",
    "x = np.linspace(0, 1, 1000)\n",
    "y = tri_dist.pdf(x)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x, y, 'b-', linewidth=2, label='Triangular prior')\n",
    "plt.axvline(min_val, color='red', linestyle='--', alpha=0.5, label=f'Min: {min_val*100:.0f}%')\n",
    "plt.axvline(mode_val, color='green', linestyle='--', alpha=0.5, label=f'Mode: {mode_val*100:.0f}%')\n",
    "plt.axvline(max_val, color='red', linestyle='--', alpha=0.5, label=f'Max: {max_val*100:.0f}%')\n",
    "plt.axvline(tri_mean, color='blue', linestyle=':', alpha=0.5, label=f'Mean: {tri_mean:.3f}')\n",
    "plt.fill_between(x, y, alpha=0.3)\n",
    "plt.xlabel('Proportion of Earth covered by Atlantic Ocean', fontsize=12)\n",
    "plt.ylabel('Probability density', fontsize=12)\n",
    "#plt.title('Elicited Prior: Triangular Distribution', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.xlim(0, 1)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig('triangular2.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median\n",
    "\n",
    "The median depends on whether the mode is below or above the midpoint:\n",
    "\n",
    "$$\\text{Median} = \\begin{cases}\n",
    "\\text{min} + \\sqrt{\\frac{(\\text{max}-\\text{min})(\\text{mode}-\\text{min})}{2}} & \\text{if } \\text{mode} \\geq \\frac{\\text{min}+\\text{max}}{2} \\\\[10pt]\n",
    "\\text{max} - \\sqrt{\\frac{(\\text{max}-\\text{min})(\\text{max}-\\text{mode})}{2}} & \\text{if } \\text{mode} < \\frac{\\text{min}+\\text{max}}{2}\n",
    "\\end{cases}$$\n",
    "\n",
    "**Intuition**: The median is the value that splits the area under the triangle in half.\n",
    "\n",
    "**Example**: For $\\text{min} = 0.10$, $\\text{mode} = 0.15$, $\\text{max} = 0.30$:\n",
    "\n",
    "First, find the midpoint:\n",
    "$$\\frac{\\text{min} + \\text{max}}{2} = \\frac{0.10 + 0.30}{2} = 0.20$$\n",
    "\n",
    "Since $\\text{mode} = 0.15 < 0.20$, we use the second formula:\n",
    "\n",
    "$$\\text{Median} = \\text{max} - \\sqrt{\\frac{(\\text{max}-\\text{min})(\\text{max}-\\text{mode})}{2}}$$\n",
    "\n",
    "$$= 0.30 - \\sqrt{\\frac{(0.30-0.10)(0.30-0.15)}{2}}$$\n",
    "\n",
    "$$= 0.30 - \\sqrt{\\frac{(0.20)(0.15)}{2}}$$\n",
    "\n",
    "$$= 0.30 - \\sqrt{\\frac{0.03}{2}}$$\n",
    "\n",
    "$$= 0.30 - \\sqrt{0.015}$$\n",
    "\n",
    "$$= 0.30 - 0.122 = 0.178$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the elicited prior\n",
    "x = np.linspace(0, 1, 1000)\n",
    "y = tri_dist.pdf(x)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x, y, 'b-', linewidth=2, label='Triangular prior')\n",
    "plt.axvline(min_val, color='red', linestyle='--', alpha=0.5, label=f'Min: {min_val*100:.0f}%')\n",
    "plt.axvline(mode_val, color='green', linestyle='--', alpha=0.5, label=f'Mode: {mode_val*100:.0f}%')\n",
    "plt.axvline(max_val, color='red', linestyle='--', alpha=0.5, label=f'Max: {max_val*100:.0f}%')\n",
    "plt.axvline(tri_mean, color='blue', linestyle=':', alpha=0.5, label=f'Mean: {tri_mean:.3f}')\n",
    "plt.axvline(tri_median, color='blue', linestyle='--', alpha=0.5, label=f'Median: {tri_median:.3f}')\n",
    "plt.fill_between(x, y, alpha=0.3)\n",
    "plt.xlabel('Proportion of Earth covered by Atlantic Ocean', fontsize=12)\n",
    "plt.ylabel('Probability density', fontsize=12)\n",
    "#plt.title('Elicited Prior: Triangular Distribution', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.xlim(0, 1)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig('triangular3.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviation\n",
    "\n",
    "The variance of a triangular distribution is:\n",
    "\n",
    "$$\\sigma^2 = \\frac{\\text{min}^2 + \\text{max}^2 + \\text{mode}^2 - \\text{min} \\cdot \\text{max} - \\text{min} \\cdot \\text{mode} - \\text{max} \\cdot \\text{mode}}{18}$$\n",
    "\n",
    "And the standard deviation is:\n",
    "\n",
    "$$\\sigma = \\sqrt{\\frac{\\text{min}^2 + \\text{max}^2 + \\text{mode}^2 - \\text{min} \\cdot \\text{max} - \\text{min} \\cdot \\text{mode} - \\text{max} \\cdot \\text{mode}}{18}}$$\n",
    "\n",
    "**Intuition**: The spread increases as the range $(\\text{max} - \\text{min})$ increases and as the mode moves away from the center.\n",
    "\n",
    "**Example**: For $\\text{min} = 0.10$, $\\text{mode} = 0.15$, $\\text{max} = 0.30$:\n",
    "\n",
    "First, calculate each term:\n",
    "- $\\text{min}^2 = (0.10)^2 = 0.01$\n",
    "- $\\text{max}^2 = (0.30)^2 = 0.09$\n",
    "- $\\text{mode}^2 = (0.15)^2 = 0.0225$\n",
    "- $\\text{min} \\cdot \\text{max} = (0.10)(0.30) = 0.03$\n",
    "- $\\text{min} \\cdot \\text{mode} = (0.10)(0.15) = 0.015$\n",
    "- $\\text{max} \\cdot \\text{mode} = (0.30)(0.15) = 0.045$\n",
    "\n",
    "Now substitute:\n",
    "\n",
    "$$\\sigma^2 = \\frac{0.01 + 0.09 + 0.0225 - 0.03 - 0.015 - 0.045}{18}$$\n",
    "\n",
    "$$= \\frac{0.1225 - 0.09}{18}$$\n",
    "\n",
    "$$= \\frac{0.0325}{18} = 0.00181$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\\sigma = \\sqrt{0.00181} = 0.043$$\n",
    "\n",
    "\n",
    "### Properties to Remember\n",
    "\n",
    "1. **Mean is intuitive**: Just average the three values\n",
    "2. **Symmetric case**: When $\\text{mode} = \\frac{\\text{min}+\\text{max}}{2}$ (mode at center), the mean, median, and mode all coincide\n",
    "3. **Skewness**: When mode is closer to min, the distribution is right-skewed; when closer to max, it's left-skewed\n",
    "4. **Maximum standard deviation**: Occurs when the mode is at one of the extremes ($\\text{mode} = \\text{min}$ or $\\text{mode} = \\text{max}$), giving $\\sigma = \\frac{\\text{max}-\\text{min}}{\\sqrt{18}} \\approx 0.236(\\text{max}-\\text{min})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the elicited prior\n",
    "x = np.linspace(0, 1, 1000)\n",
    "y = tri_dist.pdf(x)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x, y, 'b-', linewidth=2, label='Triangular prior')\n",
    "plt.axvline(min_val, color='red', linestyle='--', alpha=0.5, label=f'Min: {min_val*100:.0f}%')\n",
    "plt.axvline(mode_val, color='green', linestyle='--', alpha=0.5, label=f'Mode: {mode_val*100:.0f}%')\n",
    "plt.axvline(max_val, color='red', linestyle='--', alpha=0.5, label=f'Max: {max_val*100:.0f}%')\n",
    "plt.axvline(tri_mean, color='blue', linestyle=':', alpha=0.5, label=f'Mean: {tri_mean:.3f}')\n",
    "plt.axvline(tri_median, color='blue', linestyle='--', alpha=0.5, label=f'Median: {tri_median:.3f}')\n",
    "plt.axvline(tri_mean+2*tri_std, color='blue', linestyle='-', alpha=0.5, label=f'~L95: {tri_mean+2*tri_std:.3f}')\n",
    "plt.axvline(tri_mean-2*tri_std, color='blue', linestyle='-', alpha=0.5, label=f'~U95: {tri_median-2*tri_std:.3f}')\n",
    "plt.fill_between(x, y, alpha=0.3)\n",
    "plt.xlabel('Proportion of Earth covered by Atlantic Ocean', fontsize=12)\n",
    "plt.ylabel('Probability density', fontsize=12)\n",
    "#plt.title('Elicited Prior: Triangular Distribution', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.xlim(0, 1)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig('triangular4.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to a Beta Distribution\n",
    "    \n",
    "While the triangular distribution is great for elicitation, the **Beta distribution** is more commonly used as a prior for proportions in Bayesian analysis. The Beta distribution is the conjugate prior for the binomial likelihood, which makes it computationally convenient and provides nice interpretability.\n",
    "\n",
    "We can match the moments (mean and variance) of our triangular distribution to find equivalent Beta parameters:\n",
    "\n",
    "From the formulas:\n",
    "\n",
    "$$\\mu = \\frac{\\alpha}{\\alpha + \\beta}$$\n",
    "\n",
    "$$\\sigma^2 = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}$$\n",
    "\n",
    "We can derive:\n",
    "\n",
    "$$\\text{common} = \\frac{\\mu(1-\\mu)}{\\sigma^2} - 1$$\n",
    "\n",
    "$$\\alpha = \\mu \\cdot \\text{common}$$\n",
    "\n",
    "$$\\beta = (1-\\mu) \\cdot \\text{common}$$\n",
    "\n",
    "so given the triangular distribution with:\n",
    "- $\\mu = 0.183$ (mean)\n",
    "- $\\sigma = 0.043$ (standard deviation)\n",
    "- $\\sigma^2 = (0.043)^2 = 0.001849$ (variance)\n",
    "\n",
    "### Step 1: Calculate common\n",
    "\n",
    "$$\\text{common} = \\frac{\\mu(1-\\mu)}{\\sigma^2} - 1$$\n",
    "\n",
    "$$= \\frac{0.183 \\times (1 - 0.183)}{0.001849} - 1$$\n",
    "\n",
    "$$= \\frac{0.183 \\times 0.817}{0.001849} - 1$$\n",
    "\n",
    "$$= \\frac{0.1495}{0.001849} - 1$$\n",
    "\n",
    "$$= 80.86 - 1$$\n",
    "\n",
    "$$= 79.86$$\n",
    "\n",
    "### Step 2: Calculate α\n",
    "\n",
    "$$\\alpha = \\mu \\cdot \\text{common}$$\n",
    "\n",
    "$$= 0.183 \\times 79.86$$\n",
    "\n",
    "$$= 14.61$$\n",
    "\n",
    "### Step 3: Calculate β\n",
    "\n",
    "$$\\beta = (1-\\mu) \\cdot \\text{common}$$\n",
    "\n",
    "$$= 0.817 \\times 79.86$$\n",
    "\n",
    "$$= 65.25$$\n",
    "\n",
    "### Result\n",
    "\n",
    "The equivalent Beta distribution is: **Beta(14.61, 65.25)**\n",
    "\n",
    "### Verification\n",
    "\n",
    "Check that we get back the original moments:\n",
    "\n",
    "**Mean:**\n",
    "$$\\mu = \\frac{\\alpha}{\\alpha + \\beta} = \\frac{14.61}{14.61 + 65.25} = \\frac{14.61}{79.86} = 0.183 \\,\\checkmark$$\n",
    "\n",
    "**Variance:**\n",
    "$$\\sigma^2 = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)} = \\frac{14.61 \\times 65.25}{(79.86)^2 \\times 80.86}$$\n",
    "\n",
    "$$= \\frac{953.30}{6,377.62 \\times 80.86} = \\frac{953.30}{515,794} = 0.001848 \\,\\checkmark$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method of moments: match mean and variance\n",
    "# For Beta(α, β): mean = α/(α+β), var = αβ/[(α+β)²(α+β+1)]\n",
    "\n",
    "def beta_params_from_moments(mean, var):\n",
    "    \"\"\"\n",
    "    Convert mean and variance to Beta distribution parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    mean: float, mean of the distribution (between 0 and 1)\n",
    "    var: float, variance of the distribution\n",
    "    \n",
    "    Returns:\n",
    "    alpha, beta: shape parameters for Beta distribution\n",
    "    \"\"\"\n",
    "    # From the formulas:\n",
    "    # mean = α/(α+β)\n",
    "    # var = αβ/[(α+β)²(α+β+1)]\n",
    "    # We can derive:\n",
    "    common = mean * (1 - mean) / var - 1\n",
    "    alpha = mean * common\n",
    "    beta = (1 - mean) * common\n",
    "    return alpha, beta\n",
    "\n",
    "# Get Beta parameters matching our triangular prior\n",
    "alpha, beta = beta_params_from_moments(tri_mean, tri_std**2)\n",
    "\n",
    "print(f\"Beta distribution parameters matching triangular prior:\")\n",
    "print(f\"  α (alpha) = {alpha:.3f}\")\n",
    "print(f\"  β (beta)  = {beta:.3f}\")\n",
    "\n",
    "# Create the Beta distribution\n",
    "beta_dist = stats.beta(alpha, beta)\n",
    "\n",
    "# Verify the match\n",
    "beta_mean = beta_dist.mean()\n",
    "beta_std = beta_dist.std()\n",
    "\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"  Beta mean: {beta_mean:.3f} (Triangular mean: {tri_mean:.3f})\")\n",
    "print(f\"  Beta std:  {beta_std:.3f} (Triangular std: {tri_std:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the distributions\n",
    "x = np.linspace(0, 1, 1000)\n",
    "y_tri = tri_dist.pdf(x)\n",
    "y_beta = beta_dist.pdf(x)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x, y_tri, 'b-', linewidth=2, alpha=0.7, label='Triangular (elicited)')\n",
    "plt.plot(x, y_beta, 'r--', linewidth=2, alpha=0.7, label=f'Beta({alpha:.2f}, {beta:.2f})')\n",
    "plt.fill_between(x, y_tri, alpha=0.2, color='blue')\n",
    "plt.fill_between(x, y_beta, alpha=0.2, color='red')\n",
    "plt.axvline(tri_mean, color='blue', linestyle=':', alpha=0.5, label=f'Mean: {tri_mean:.3f}')\n",
    "plt.xlabel('Proportion of Earth covered by Atlantic Ocean', fontsize=12)\n",
    "plt.ylabel('Probability density', fontsize=12)\n",
    "#plt.title('Triangular vs Beta Distribution', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.xlim(0, 1)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig('triangular_beta.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliciting from Multiple People and Creating a Joint Prior\n",
    "\n",
    "In real-world applications, we often want to gather prior information from multiple sources - different students, multiple experts, or various stakeholders. Rather than relying on a single person's belief, pooling multiple opinions creates a more robust prior.\n",
    "\n",
    "Let's ask people for their beliefs about the coverage of the **Pacific Ocean**; each person provides their own minimum, mode, and maximum estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Elicitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== EDIT THESE VALUES ==========\n",
    "# Each row is [minimum, mode, maximum] for one person's belief\n",
    "# Values should be between 0 and 1 (proportions, not percentages)\n",
    "\n",
    "elicitations = np.array([\n",
    "    # Person   Min    Mode   Max\n",
    "    [0.15, 0.30, 0.60],  # Person 1: Very uncertain, wide range\n",
    "    [0.25, 0.35, 0.45],  # Person 2: More confident, narrower range\n",
    "    [0.30, 0.45, 0.60],  # Person 3: Thinks it's quite large\n",
    "    [0.20, 0.25, 0.35],  # Person 4: Conservative, lower estimate\n",
    "    [0.25, 0.33, 0.50],  # Person 5: Moderate uncertainty\n",
    "    [0.30, 0.40, 0.55],  # Person 6: Higher estimate\n",
    "    [0.18, 0.28, 0.40],  # Person 7: Lower estimate\n",
    "    [0.26, 0.30, 0.36],  # Person 8: Very confident, tight range\n",
    "    [0.20, 0.35, 0.55],  # Person 9: Wide uncertainty\n",
    "    [0.24, 0.32, 0.42],  # Person 10: Moderate confidence\n",
    "])\n",
    "\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Individual Triangular Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create triangular distributions for each person\n",
    "individual_dists = []\n",
    "\n",
    "for i, (min_val, mode_val, max_val) in enumerate(elicitations):\n",
    "    c = (mode_val - min_val) / (max_val - min_val)\n",
    "    dist = stats.triang(c=c, loc=min_val, scale=max_val - min_val)\n",
    "    individual_dists.append(dist)\n",
    "\n",
    "print(f\"Created {len(individual_dists)} triangular distributions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all individual priors\n",
    "x = np.linspace(0, 1, 1000)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, n_people))\n",
    "\n",
    "for i, dist in enumerate(individual_dists):\n",
    "    y = dist.pdf(x)\n",
    "    plt.plot(x, y, linewidth=1.5, alpha=0.6, color=colors[i], label=f'Person {i+1}')\n",
    "    plt.fill_between(x, y, alpha=0.1, color=colors[i])\n",
    "\n",
    "plt.xlabel('Proportion of Earth covered by Pacific Ocean', fontsize=12)\n",
    "plt.ylabel('Probability density', fontsize=12)\n",
    "plt.title(f'Individual Elicited Priors (n={n_people})', fontsize=14)\n",
    "plt.legend(ncol=2, fontsize=9, loc='upper right')\n",
    "plt.xlim(0, 1)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig('multi_triangular.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pool the Elicitations into a Joint Beta Prior\n",
    "\n",
    "Now we combine these individual beliefs. We'll use **moment matching** again to pool the means and variances from all individuals, then fit a single Beta distribution that captures the consensus.\n",
    "\n",
    "This approach:\n",
    "- Captures the central tendency across all experts\n",
    "- Accounts for both within-person uncertainty and between-person disagreement\n",
    "- Produces a conjugate Beta prior for convenient Bayesian updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and variance for each person's triangular distribution\n",
    "individual_means = [dist.mean() for dist in individual_dists]\n",
    "individual_vars = [dist.var() for dist in individual_dists]\n",
    "\n",
    "# Pool the statistics\n",
    "# Mean: Average across all people\n",
    "pooled_mean = np.mean(individual_means)\n",
    "\n",
    "# Variance: Average within-person variance + between-person variance\n",
    "# This captures both individual uncertainty AND disagreement between people\n",
    "within_person_var = np.mean(individual_vars)\n",
    "between_person_var = np.var(individual_means)\n",
    "pooled_var = within_person_var + between_person_var\n",
    "\n",
    "print(f\"Pooling statistics:\")\n",
    "print(f\"  Average of individual means: {pooled_mean:.3f}\")\n",
    "print(f\"  Within-person uncertainty:   {within_person_var:.4f}\")\n",
    "print(f\"  Between-person disagreement: {between_person_var:.4f}\")\n",
    "print(f\"  Total pooled variance:       {pooled_var:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pooled moments to Beta parameters\n",
    "alpha_pooled, beta_pooled = beta_params_from_moments(pooled_mean, pooled_var)\n",
    "joint_prior = stats.beta(alpha_pooled, beta_pooled)\n",
    "\n",
    "print(f\"Joint prior (pooled from {n_people} people):\")\n",
    "print(f\"  Beta({alpha_pooled:.3f}, {beta_pooled:.3f})\")\n",
    "print(f\"\\nJoint prior statistics:\")\n",
    "print(f\"  Mean: {joint_prior.mean():.3f} ({joint_prior.mean()*100:.1f}%)\")\n",
    "print(f\"  Std:  {joint_prior.std():.3f} ({joint_prior.std()*100:.1f}%)\")\n",
    "print(f\"  Median: {joint_prior.median():.3f}\")\n",
    "print(f\"  95% credible interval: [{joint_prior.ppf(0.025):.3f}, {joint_prior.ppf(0.975):.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the joint prior against individual priors\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: All individual priors (faint) with joint prior\n",
    "for i, dist in enumerate(individual_dists):\n",
    "    axes[0].plot(x, dist.pdf(x), linewidth=0.8, alpha=0.3, color='gray')\n",
    "axes[0].plot(x, joint_prior.pdf(x), 'darkblue', linewidth=3, \n",
    "             label=f'Joint Prior: Beta({alpha_pooled:.2f}, {beta_pooled:.2f})')\n",
    "axes[0].fill_between(x, joint_prior.pdf(x), alpha=0.3, color='darkblue')\n",
    "axes[0].axvline(pooled_mean, color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Mean: {pooled_mean:.3f}', alpha=0.7)\n",
    "axes[0].set_xlabel('Proportion', fontsize=12)\n",
    "axes[0].set_ylabel('Density', fontsize=12)\n",
    "axes[0].set_title('Joint Prior from Pooled Elicitations', fontsize=13)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].set_xlim(0, 1)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Right: Comparison showing spread\n",
    "axes[1].plot(x, joint_prior.pdf(x), 'darkblue', linewidth=3, label='Joint prior')\n",
    "axes[1].fill_between(x, joint_prior.pdf(x), alpha=0.2, color='darkblue')\n",
    "\n",
    "# Show the range of individual means\n",
    "for mean_i in individual_means:\n",
    "    axes[1].axvline(mean_i, color='orange', alpha=0.3, linewidth=1)\n",
    "axes[1].axvline(individual_means[0], color='orange', alpha=0.3, linewidth=1, \n",
    "                label='Individual means')\n",
    "\n",
    "axes[1].axvline(pooled_mean, color='red', linestyle='--', linewidth=2.5, \n",
    "                label=f'Pooled mean: {pooled_mean:.3f}')\n",
    "\n",
    "# Add credible interval\n",
    "ci_lower = joint_prior.ppf(0.025)\n",
    "ci_upper = joint_prior.ppf(0.975)\n",
    "axes[1].axvspan(ci_lower, ci_upper, alpha=0.2, color='green', \n",
    "                label=f'95% CI: [{ci_lower:.3f}, {ci_upper:.3f}]')\n",
    "\n",
    "axes[1].set_xlabel('Proportion', fontsize=12)\n",
    "axes[1].set_ylabel('Density', fontsize=12)\n",
    "axes[1].set_title('Joint Prior with Individual Variation', fontsize=13)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].set_xlim(0, 1)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('multi_beta.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint prior represents our best synthesis of the collective knowledge from all {n_people} people. The spread reflects both:\n",
    "1. **Individual uncertainty**: How uncertain each person was (width of their triangular distributions)\n",
    "2. **Expert disagreement**: How much people disagreed with each other (spread of their modal estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Updating with Observed Data\n",
    "\n",
    "Now let's use our joint prior for Bayesian inference. Imagine we collect some actual data by randomly tossing a globe and recording whether our finger lands on the Pacific Ocean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate some data\n",
    "# True value: Pacific Ocean is about 32% of Earth's surface (165.25M km² / 510M km²)\n",
    "true_proportion = 0.32\n",
    "\n",
    "# Simulate 50 globe tosses\n",
    "n_tosses = 50\n",
    "np.random.seed(42)\n",
    "tosses = np.random.binomial(1, true_proportion, n_tosses)\n",
    "n_pacific = tosses.sum()\n",
    "\n",
    "print(f\"Observed data:\")\n",
    "print(f\"  Number of tosses: {n_tosses}\")\n",
    "print(f\"  Landed on Pacific: {n_pacific}\")\n",
    "print(f\"  Sample proportion: {n_pacific/n_tosses:.3f}\")\n",
    "print(f\"  True proportion: {true_proportion:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian updating: Beta-Binomial conjugacy\n",
    "# Prior: Beta(α, β)\n",
    "# Likelihood: Binomial(n, p) with k successes\n",
    "# Posterior: Beta(α + k, β + n - k)\n",
    "\n",
    "alpha_post = alpha_pooled + n_pacific\n",
    "beta_post = beta_pooled + (n_tosses - n_pacific)\n",
    "posterior = stats.beta(alpha_post, beta_post)\n",
    "\n",
    "print(f\"Posterior: Beta({alpha_post:.2f}, {beta_post:.2f})\")\n",
    "print(f\"\\nPosterior statistics:\")\n",
    "print(f\"  Mean: {posterior.mean():.3f}\")\n",
    "print(f\"  Std:  {posterior.std():.3f}\")\n",
    "print(f\"  95% credible interval: [{posterior.ppf(0.025):.3f}, {posterior.ppf(0.975):.3f}]\")\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  Prior mean (from {n_people} people): {joint_prior.mean():.3f}\")\n",
    "print(f\"  Sample proportion:               {n_pacific/n_tosses:.3f}\")\n",
    "print(f\"  Posterior mean:                  {posterior.mean():.3f}\")\n",
    "print(f\"  True value:                      {true_proportion:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the updating process\n",
    "x = np.linspace(0, 1, 1000)\n",
    "\n",
    "# Prior (from pooled elicitations)\n",
    "y_prior = joint_prior.pdf(x)\n",
    "\n",
    "# Likelihood (scaled for visualization)\n",
    "likelihood = stats.binom(n_tosses, x).pmf(n_pacific)\n",
    "likelihood_scaled = likelihood / likelihood.max() * y_prior.max() * 0.8\n",
    "\n",
    "# Posterior\n",
    "y_post = posterior.pdf(x)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(x, y_prior, 'b-', linewidth=2.5, \n",
    "         label=f'Prior: Beta({alpha_pooled:.2f}, {beta_pooled:.2f})\\n(from {n_people} elicitations)', \n",
    "         alpha=0.8)\n",
    "plt.plot(x, likelihood_scaled, 'g-', linewidth=2, \n",
    "         label=f'Likelihood (scaled)\\n({n_pacific}/{n_tosses} Pacific)', \n",
    "         alpha=0.7)\n",
    "plt.plot(x, y_post, 'r-', linewidth=3, \n",
    "         label=f'Posterior: Beta({alpha_post:.2f}, {beta_post:.2f})')\n",
    "plt.axvline(true_proportion, color='black', linestyle='--', linewidth=2, \n",
    "            label=f'True value: {true_proportion:.3f}', alpha=0.7)\n",
    "plt.fill_between(x, y_post, alpha=0.2, color='red')\n",
    "\n",
    "# Add vertical lines for means\n",
    "plt.axvline(joint_prior.mean(), color='blue', linestyle=':', linewidth=1.5, alpha=0.5)\n",
    "plt.axvline(n_pacific/n_tosses, color='green', linestyle=':', linewidth=1.5, alpha=0.5)\n",
    "plt.axvline(posterior.mean(), color='red', linestyle=':', linewidth=1.5, alpha=0.5)\n",
    "\n",
    "plt.xlabel('Proportion of Earth covered by Pacific Ocean', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.legend(fontsize=10, loc='upper right')\n",
    "plt.xlim(0, 0.8)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig('multi_beta2.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior represents a synthesis of:\n",
    "1. **Prior knowledge**: The pooled beliefs of {n_people} people\n",
    "2. **New data**: {n_tosses} globe tosses with observed outcomes\n",
    "\n",
    "Notice how the posterior mean is a weighted average between the prior mean and the sample proportion. With more data, the posterior would increasingly reflect the likelihood and less the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-Odds Transformation for Logistic Regression\n",
    "\n",
    "In logistic regression and many GLMs, we work with log-odds (logit) rather than probabilities directly. The logit transformation is:\n",
    "\n",
    "$$\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)$$\n",
    "\n",
    "This maps probabilities from (0,1) to the entire real line (-∞, +∞), which is useful when we want to use normal priors in our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define logit and inverse logit functions\n",
    "def logit(p):\n",
    "    \"\"\"Convert probability to log-odds.\"\"\"\n",
    "    return np.log(p / (1 - p))\n",
    "\n",
    "def inv_logit(x):\n",
    "    \"\"\"Convert log-odds to probability.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Transform our joint Beta prior to log-odds scale\n",
    "# Sample from Beta, transform to logit\n",
    "n_samples = 10000\n",
    "beta_samples = joint_prior.rvs(n_samples)\n",
    "logit_samples = logit(beta_samples)\n",
    "\n",
    "print(f\"Log-odds transformation of Beta({alpha_pooled:.2f}, {beta_pooled:.2f}):\")\n",
    "print(f\"  Mean log-odds: {logit_samples.mean():.3f}\")\n",
    "print(f\"  Std log-odds:  {logit_samples.std():.3f}\")\n",
    "print(f\"\\nFor reference:\")\n",
    "print(f\"  logit(0.25) = {logit(0.25):.3f}\")\n",
    "print(f\"  logit(0.33) = {logit(0.33):.3f}\")\n",
    "print(f\"  logit(0.50) = {logit(0.50):.3f}\")\n",
    "print(f\"  logit(0.75) = {logit(0.75):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the transformed distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left panel: probability scale\n",
    "axes[0].hist(beta_samples, bins=50, density=True, alpha=0.6, color='blue', edgecolor='black')\n",
    "x_prob = np.linspace(0, 1, 1000)\n",
    "axes[0].plot(x_prob, joint_prior.pdf(x_prob), 'r-', linewidth=2, label='Beta PDF')\n",
    "axes[0].axvline(pooled_mean, color='green', linestyle='--', linewidth=2, \n",
    "                label=f'Mean: {pooled_mean:.3f}')\n",
    "axes[0].set_xlabel('Proportion (probability scale)', fontsize=12)\n",
    "axes[0].set_ylabel('Density', fontsize=12)\n",
    "axes[0].set_title('Joint Beta Prior (Probability Scale)', fontsize=13)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Right panel: log-odds scale\n",
    "axes[1].hist(logit_samples, bins=50, density=True, alpha=0.6, color='purple', edgecolor='black')\n",
    "axes[1].axvline(logit_samples.mean(), color='green', linestyle='--', linewidth=2, \n",
    "                label=f'Mean: {logit_samples.mean():.3f}')\n",
    "axes[1].axvline(0, color='black', linestyle=':', alpha=0.5, label='logit(0.5) = 0')\n",
    "axes[1].set_xlabel('Log-odds (logit scale)', fontsize=12)\n",
    "axes[1].set_ylabel('Density', fontsize=12)\n",
    "axes[1].set_title('Transformed Distribution (Log-Odds Scale)', fontsize=13)\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-odds transformation creates an approximately normal distribution. This is useful because we can now approximate this with a normal prior in logistic regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a normal distribution to the log-odds\n",
    "from scipy.stats import norm\n",
    "\n",
    "logit_mean = logit_samples.mean()\n",
    "logit_std = logit_samples.std()\n",
    "\n",
    "# Create a normal distribution with these parameters\n",
    "logit_normal = norm(logit_mean, logit_std)\n",
    "\n",
    "print(f\"Normal approximation in log-odds space:\")\n",
    "print(f\"  N({logit_mean:.3f}, {logit_std:.3f}²)\")\n",
    "print(f\"\\nThis can be used as a prior in logistic regression:\")\n",
    "print(f\"\\nIn PyMC:\")\n",
    "print(f\"  intercept = pm.Normal('intercept', mu={logit_mean:.3f}, sigma={logit_std:.3f})\")\n",
    "print(f\"\\nIn Stan:\")\n",
    "print(f\"  intercept ~ normal({logit_mean:.3f}, {logit_std:.3f});\")\n",
    "print(f\"\\nIn R (brms):\")\n",
    "print(f\"  prior(normal({logit_mean:.3f}, {logit_std:.3f}), class = Intercept)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare log-odds transformation vs normal approximation\n",
    "x_logit = np.linspace(-3, 2, 1000)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(logit_samples, bins=50, density=True, alpha=0.4, color='purple', \n",
    "         label='Transformed Beta samples', edgecolor='black')\n",
    "plt.plot(x_logit, logit_normal.pdf(x_logit), 'r-', linewidth=2.5, \n",
    "         label=f'Normal({logit_mean:.3f}, {logit_std:.3f})')\n",
    "plt.axvline(logit_mean, color='green', linestyle='--', linewidth=2, alpha=0.7,\n",
    "            label=f'Mean: {logit_mean:.3f}')\n",
    "plt.axvline(0, color='black', linestyle=':', alpha=0.5)\n",
    "plt.xlabel('Log-odds', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.title('Log-Odds Prior: Transformed Beta vs Normal Approximation', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Add second x-axis showing probability scale\n",
    "ax2 = plt.gca().twiny()\n",
    "ax2.set_xlim(plt.gca().get_xlim())\n",
    "logit_ticks = np.array([-2, -1, 0, 1])\n",
    "prob_ticks = inv_logit(logit_ticks)\n",
    "ax2.set_xticks(logit_ticks)\n",
    "ax2.set_xticklabels([f'{p:.2f}' for p in prob_ticks])\n",
    "ax2.set_xlabel('Corresponding probability', fontsize=12, color='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implied sample size\n",
    "\n",
    "Beside eliciting bounds or ranges of things, there are many other ways of getting at the underlying (implied) paramers of a probability distribution. One highlighted by O'Hagan *et al.* is the *equivalent prior sample* (EST) method, which recognizes that elicition methods often make the uncertainties too low, even for an opinion from one person. \n",
    "\n",
    "The EST method asks for a point estimate of the expected value (in my case, 0.72) but also then asks \"based on how many samples?\" Then given $n$ the calculation can be made that $\\alpha = n\\hat{p}$ and $\\beta = n(1-\\hat{p})$.\n",
    "\n",
    "For examples about the earth, there is only one, so the resulting pdf would simply be $Beta(0.72, 0.18)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate points for the Beta distribution PDF\n",
    "pdf_y = sp.stats.beta.pdf(x, 0.72, 0.18)\n",
    "\n",
    "# Plot the Beta distribution PDF\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x, pdf_y, label=\"Beta PDF (α=0.72, β=0.18)\", color='green')\n",
    "\n",
    "# Add graph details\n",
    "plt.title(\"Some Beta Distribution PDF\", fontsize=14)\n",
    "plt.xlabel(\"Probability\", fontsize=12)\n",
    "plt.ylabel(\"PDF Value\", fontsize=12)\n",
    "plt.axhline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "plt.axvline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.4)\n",
    "plt.savefig('one_earth.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is a bit of a crap estimate. But let's instead use samples from people to give some perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate points for the Beta distribution PDF\n",
    "\n",
    "# Plot the Beta distribution PDF\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x, sp.stats.beta.pdf(x, 5*0.72, 0.18), label=\"Beta PDF (α=0.72, β=0.18)\")\n",
    "plt.plot(x, sp.stats.beta.pdf(x, 5*0.72, 5*0.18), label=\"Beta PDF  (α=5x0.72, β=5x0.18)\")\n",
    "plt.plot(x, sp.stats.beta.pdf(x, 10*0.72, 10*0.18), label=\"Beta PDF  (α=10x0.72, β=10x0.18)\")\n",
    "plt.plot(x, sp.stats.beta.pdf(x, 50*0.72, 50*0.18), label=\"Beta PDF  (α=50x0.72, β=50x0.18)\")\n",
    "plt.plot(x, sp.stats.beta.pdf(x, 500*0.72, 500*0.18), label=\"Beta PDF  (α=500x0.72, β=500x0.18)\")\n",
    "\n",
    "\n",
    "# Add graph details\n",
    "plt.title(\"Some Beta Distributions PDF\", fontsize=14)\n",
    "plt.xlabel(\"Probability\", fontsize=12)\n",
    "plt.ylabel(\"PDF Value\", fontsize=12)\n",
    "plt.axhline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "plt.axvline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.4)\n",
    "plt.savefig('multi_earth.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, this looks worse than what we have using the CDFs with few numbers of people and in practice it frequently is - the CDF is just better. But good to know that there are other options.\n",
    "\n",
    "While the elicitation of probabilities is good in the sense it is bounded and therefore tractable, what about paramters in a geocentric linear model? How can people elicit such things? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate expert responses\n",
    "# Assume three experts provide their mean and confidence intervals for the slope\n",
    "expert_responses = {\n",
    "    \"Expert 1\": {\"mean\": 2.0, \"std\": 0.5},  # Mean and standard deviation\n",
    "    \"Expert 2\": {\"mean\": 2.2, \"std\": 0.3},\n",
    "    \"Expert 3\": {\"mean\": 1.8, \"std\": 0.4},\n",
    "}\n",
    "\n",
    "# Simulate individual distributions\n",
    "n_samples = 1000\n",
    "x = np.linspace(0, 4, n_samples)  # Range for the slope\n",
    "distributions = {\n",
    "    name: sp.stats.norm.pdf(x, loc=data[\"mean\"], scale=data[\"std\"])\n",
    "    for name, data in expert_responses.items()\n",
    "}\n",
    "\n",
    "# Aggregate expert opinions using weighted averaging\n",
    "# Equal weights for simplicity\n",
    "weights = np.array([1/3, 1/3, 1/3])\n",
    "aggregated_pdf = sum(weight * dist for weight, dist in zip(weights, distributions.values()))\n",
    "\n",
    "\n",
    "# Calculate the aggregated mean and standard deviation\n",
    "aggregated_mean = sum(weights[i] * expert_responses[f\"Expert {i+1}\"][\"mean\"] for i in range(len(weights)))\n",
    "aggregated_variance = sum(weights[i] * (expert_responses[f\"Expert {i+1}\"][\"std\"]**2 + \n",
    "                                       (expert_responses[f\"Expert {i+1}\"][\"mean\"] - aggregated_mean)**2) for i in range(len(weights)))\n",
    "aggregated_std = round(np.sqrt(aggregated_variance),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot individual expert distributions and the aggregated distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "for name, pdf in distributions.items():\n",
    "    plt.plot(x, pdf, label=f\"{name} (Mean: {expert_responses[name]['mean']}, Std: {expert_responses[name]['std']})\")\n",
    "plt.plot(x, aggregated_pdf, label=f\"{'Aggregated'} (Mean: {aggregated_mean}, Std: {aggregated_std})\", color=\"black\", lw=2, linestyle=\"--\")\n",
    "\n",
    "# Add plot details\n",
    "plt.title(\"Direct elicitation of Slope in Simple Linear Regression\", fontsize=14)\n",
    "plt.xlabel(\"Slope\", fontsize=12)\n",
    "plt.ylabel(\"Density\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.4)\n",
    "\n",
    "plt.savefig('direct.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is fine if we have experts that know something about - and can think in terms of - the mean and standard deviation of a regression slope. But what about normal people? Well first the language has to be good - saying what is the slope isn't good, but saying \"what is the most change you would expect in Y given a change from X1 to X2?\" (with context appropriate words for Y, X1 and X2)...\"and what is the minimum change you would expect?\" And \"How sure are you that the change would be within this range?\"\n",
    "\n",
    "With these statements in hand we can convert into quantitative estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experts provide a plausible range (lower and upper bounds) with a confidence level (e.g., 95%)\n",
    "expert_ranges = {\n",
    "    \"Expert 1\": {\"lower\": 1.5, \"upper\": 2.5, \"confidence\": 0.90},\n",
    "    \"Expert 2\": {\"lower\": 2.0, \"upper\": 2.4, \"confidence\": 0.95},\n",
    "    \"Expert 3\": {\"lower\": 1.6, \"upper\": 2.0, \"confidence\": 0.75},\n",
    "}\n",
    "\n",
    "# Convert ranges to mean and std assuming a normal distribution\n",
    "for name, data in expert_ranges.items():\n",
    "    mean = (data[\"lower\"] + data[\"upper\"]) / 2\n",
    "    std = (data[\"upper\"] - data[\"lower\"]) / (2 * sp.stats.norm.ppf((1 + data[\"confidence\"]) / 2))\n",
    "    expert_ranges[name][\"mean\"] = mean\n",
    "    expert_ranges[name][\"std\"] = std\n",
    "\n",
    "# Calculate the aggregated mean from the weighted means\n",
    "aggregated_mean = sum(\n",
    "    weights[i] * expert_ranges[f\"Expert {i+1}\"][\"mean\"] for i in range(len(weights))\n",
    ")\n",
    "\n",
    "# Calculate the aggregated variance from the weighted variances\n",
    "aggregated_variance = sum(\n",
    "    weights[i] * (expert_ranges[f\"Expert {i+1}\"][\"std\"]**2 +\n",
    "                  (expert_ranges[f\"Expert {i+1}\"][\"mean\"] - aggregated_mean)**2)\n",
    "    for i in range(len(weights))\n",
    ")\n",
    "\n",
    "# Compute the aggregated standard deviation\n",
    "aggregated_std = np.sqrt(aggregated_variance)\n",
    "\n",
    "\n",
    "aggregated_pdf = sp.stats.norm.pdf(x, aggregated_mean, scale=aggregated_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot individual expert distributions and the aggregated distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "for name, pdf in distributions.items():\n",
    "    plt.plot(x, pdf, label=f\"{name} (Lower: {expert_ranges[name]['lower']}, Upper: {expert_ranges[name]['upper']}, Conf.: {expert_ranges[name]['confidence']})\")\n",
    "plt.plot(x, aggregated_pdf, label=f\"{'Aggregated'} (Mean: {aggregated_mean}, Std: {round(aggregated_std,2)})\", color=\"black\", lw=2, linestyle=\"--\")\n",
    "\n",
    "# Add plot details\n",
    "plt.title(\"Sort-of indirect elicitation of Slope in Simple Linear Regression\", fontsize=14)\n",
    "plt.xlabel(\"Slope\", fontsize=12)\n",
    "plt.ylabel(\"Density\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.4)\n",
    "\n",
    "plt.savefig('indirect.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All this is but the tip of the elicitation iceberg - there are many other, complex ways to derive estimates!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
